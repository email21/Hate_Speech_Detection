import pytorch_lightning as pl
import torch
from utils import compute_metrics
from data import prepare_dataset

from transformers import (
    AutoTokenizer,
    AutoConfig,
    AutoModelForSequenceClassification,
)
from transformers import Trainer, TrainingArguments
from transformers import EarlyStoppingCallback
from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup

from transformers import TrainerCallback


def load_tokenizer_and_model_for_train(args):
    """í•™ìŠµ(train)ì„ ìœ„í•œ ì‚¬ì „í•™ìŠµ(pretrained) í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ huggingfaceì—ì„œ load"""
    # load model and tokenizer
    MODEL_NAME = args.model_name
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # ë¹„ì‹ë³„í™”ëœ ë‹¨ì–´ í† í°ìœ¼ë¡œ ì¶”ê°€
    new_tokens = [
        "&name&",
        "&location&",
        "&affiliation&",
        "&company&",
        "&brand&",
        "&art&",
        "&other&",
        "&nama&",
        "&affifiation&",
        "&name",
        "&online-account&",
        "&compnay&",
        "&anme&",
        "& name&",
        "&address&",
        "&tel-num&",
        "&naem&",
    ]
    tokenizer.add_tokens(new_tokens)

    print(
        "í† í° ì¶”ê°€ í™•ì¸:", tokenizer.convert_tokens_to_ids(new_tokens)
    )  # í† í°ì´ ì˜ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸ # [32000, 32001, 32002, 32003, 32004, 32005, 32006, 32007, 32008, 32009, 32010, 32011, 32012, 32013, 32014, 32015, 32016]
    print(
        "ëŠ˜ì–´ë‚œ í† í° í¬ê¸°:", len(tokenizer)
    )  # í† í° í¬ê¸° ëŠ˜ì–´ë‚œ ê²ƒ í™•ì¸ # ì›ë˜ëŠ” 32000ê°œ

    # setting model hyperparameter
    model_config = AutoConfig.from_pretrained(MODEL_NAME)
    model_config.num_labels = 2
    print(model_config)

    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME, config=model_config
    )

    # ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸° ì¡°ì •(í† í°ì„ ëŠ˜ë ¸ê¸° ë•Œë¬¸ì—)
    model.resize_token_embeddings(len(tokenizer))
    print("ì„ë² ë”© ë ˆì´ì–´ í¬ê¸° ì¡°ì • ì™„ë£Œ")

    print("--- Modeling Done ---")
    return tokenizer, model


def load_model_for_inference(model_name, model_dir):
    """ì¶”ë¡ (infer)ì— í•„ìš”í•œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € load"""
    # load tokenizer
    Tokenizer_NAME = model_name
    tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)

    # ë¹„ì‹ë³„í™”ëœ ë‹¨ì–´ í† í°ìœ¼ë¡œ ì¶”ê°€
    new_tokens = [
        "&name&",
        "&location&",
        "&affiliation&",
        "&company&",
        "&brand&",
        "&art&",
        "&other&",
        "&nama&",
        "&affifiation&",
        "&name",
        "&online-account&",
        "&compnay&",
        "&anme&",
        "& name&",
        "&address&",
        "&tel-num&",
        "&naem&",
    ]
    tokenizer.add_tokens(new_tokens)

    print(
        "í† í° ì¶”ê°€ í™•ì¸2:", tokenizer.convert_tokens_to_ids(new_tokens)
    )  # í† í°ì´ ì˜ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
    print("ëŠ˜ì–´ë‚œ í† í° í¬ê¸°2:", len(tokenizer))  # í† í° í¬ê¸° ëŠ˜ì–´ë‚œ ê²ƒ í™•ì¸

    ## load my model
    model = AutoModelForSequenceClassification.from_pretrained(model_dir)

    # ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸° ì¡°ì •(í† í°ì„ ëŠ˜ë ¸ê¸° ë•Œë¬¸ì—)
    model.resize_token_embeddings(len(tokenizer))

    return tokenizer, model


def load_trainer_for_train(args, model, hate_train_dataset, hate_valid_dataset):
    """í•™ìŠµ(train)ì„ ìœ„í•œ huggingface trainer ì„¤ì •"""
    training_args = TrainingArguments(
        output_dir=args.save_path + "/results",  # output directory
        save_total_limit=args.save_limit,  # number of total save model.
        save_steps=args.save_step,  # model saving step.
        num_train_epochs=args.epochs,  # total number of training epochs
        learning_rate=args.lr,  # learning_rate
        per_device_train_batch_size=args.batch_size,  # batch size per device during training
        per_device_eval_batch_size=8,  # batch size for evaluation
        warmup_steps=args.warmup_steps,  # number of warmup steps for learning rate scheduler
        weight_decay=args.weight_decay,  # strength of weight decay
        logging_dir=args.save_path + "logs",  # directory for storing logs
        logging_steps=args.logging_step,  # log saving step.
        eval_strategy="steps",  # eval strategy to adopt during training
        # `no`: No evaluation during training.
        # `steps`: Evaluate every `eval_steps`.
        # `epoch`: Evaluate every end of epoch.
        eval_steps=args.eval_step,  # evaluation step.
        load_best_model_at_end=True,
        report_to="wandb",  # W&B ë¡œê¹… í™œì„±í™”
        run_name=args.run_name,  # run_name ì§€ì •
    )

    ## Add callback & optimizer & scheduler
    MyCallback = EarlyStoppingCallback(
        early_stopping_patience=2, early_stopping_threshold=0.001
    )

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        betas=(0.9, 0.999),
        eps=1e-08,
        weight_decay=args.weight_decay,
        amsgrad=False,
    )
    print("--- Set training arguments Done ---")

    # trainer = Trainer(
    trainer = Trainer(
        model=model,  # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,  # training arguments, defined above
        train_dataset=hate_train_dataset,  # training dataset
        eval_dataset=hate_valid_dataset,  # evaluation dataset
        compute_metrics=compute_metrics,  # define metrics function
        callbacks=[
            MyCallback,
        ],
        optimizers=(
            optimizer,
            get_cosine_with_hard_restarts_schedule_with_warmup(
                optimizer,
                num_warmup_steps=args.warmup_steps,
                num_training_steps=len(hate_train_dataset) * args.epochs,
            ),
        ),
    )

    # íŠ¸ë ˆì´ë„ˆì˜ _save ë©”ì†Œë“œë¥¼ ì§ì ‘ êµì²´í•˜ì—¬ ì˜¤ë¥˜ë¥¼ ì›ì²œ ì°¨ë‹¨í•©ë‹ˆë‹¤.
    # ==============================================================================
    original_save = trainer._save

    def new_save(output_dir=None, state_dict=None):
        # 1. ì €ì¥ ì§ì „ì— ëª¨ë“  íŒŒë¼ë¯¸í„°ì˜ ë©”ëª¨ë¦¬ë¥¼ ê°•ì œë¡œ ì—°ì†ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
        for name, param in trainer.model.named_parameters():
            if not param.is_contiguous():
                param.data = param.data.contiguous()

        # 2. ë©”ëª¨ë¦¬ ì¬ì •ë ¬ í›„, ì›ë˜ì˜ ì €ì¥ ë¡œì§ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
        original_save(output_dir, state_dict)

    trainer._save = new_save
    # ===========================================================================

    print("--- Set Trainer Done ---")

    return trainer


def train(args):
    """ëª¨ë¸ì„ í•™ìŠµ(train)í•˜ê³  best modelì„ ì €ì¥"""
    # fix a seed
    pl.seed_everything(seed=42, workers=False)

    # set device
    # device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("device:", device)

    # set model and tokenizer
    tokenizer, model = load_tokenizer_and_model_for_train(args)
    model.to(device)

    # set data
    # hate_train_dataset, hate_valid_dataset, hate_test_dataset, test_dataset = (
    #     prepare_dataset(args.dataset_dir, tokenizer, args.max_len)
    # )

    # HuggingFace ì‚¬ìš©ìœ¼ë¡œ prepare_datasetì˜ args.dataset_dir -> args.dataset_name
    hate_train_dataset, hate_valid_dataset, hate_test_dataset, test_dataset = (
        prepare_dataset(args.dataset_name, tokenizer, args.max_len, args.model_name)
    )

    # set trainer
    trainer = load_trainer_for_train(
        args, model, hate_train_dataset, hate_valid_dataset
    )

    # train model
    print("--- Start train ---")
    trainer.train()
    print("--- Finish train ---")
    model.save_pretrained(args.model_dir)
    tokenizer.save_pretrained(args.model_dir)  # í† í¬ë‚˜ì´ì € ì €ì¥
