import pytorch_lightning as pl
import torch
from utils import compute_metrics
from data import prepare_dataset, prepare_kfold_dataset, construct_tokenized_dataset, hate_dataset
import numpy as np
from sklearn.model_selection import StratifiedKFold
import os

from transformers import (
    AutoTokenizer,
    AutoConfig,
    AutoModelForSequenceClassification,
)
from transformers import Trainer, TrainingArguments
from transformers import EarlyStoppingCallback
from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup


def load_tokenizer_and_model_for_train(args):
    """í•™ìŠµ(train)ì„ ìœ„í•œ ì‚¬ì „í•™ìŠµ(pretrained) í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ huggingfaceì—ì„œ load"""
    # load model and tokenizer
    MODEL_NAME = args.model_name
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # ë¹„ì‹ë³„í™”ëœ ë‹¨ì–´ í† í°ìœ¼ë¡œ ì¶”ê°€
    new_tokens = ['&name&', '&location&', '&affiliation&', '&company&', '&brand&', '&art&', '&other&', '&nama&', '&affifiation&', '&name', '&online-account&', '&compnay&', '&anme&', '& name&', '&address&', '&tel-num&', '&naem&']
    tokenizer.add_tokens(new_tokens)
    
    print("í† í° ì¶”ê°€ í™•ì¸:", tokenizer.convert_tokens_to_ids(new_tokens)) # í† í°ì´ ì˜ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸ # [32000, 32001, 32002, 32003, 32004, 32005, 32006, 32007, 32008, 32009, 32010, 32011, 32012, 32013, 32014, 32015, 32016]
    print("ëŠ˜ì–´ë‚œ í† í° í¬ê¸°:", len(tokenizer)) # í† í° í¬ê¸° ëŠ˜ì–´ë‚œ ê²ƒ í™•ì¸ # ì›ë˜ëŠ” 32000ê°œ

    # setting model hyperparameter
    model_config = AutoConfig.from_pretrained(MODEL_NAME)
    model_config.num_labels = 2
    print(model_config)
    
    # # dropout ì¶”ê°€
    # model_config = AutoConfig.from_pretrained(MODEL_NAME)
    # model_config.num_labels = 2
    # model_config.hidden_dropout_prob = args.dropout_rate  # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ì„¤ì •
    # model_config.attention_probs_dropout_prob = args.dropout_rate

    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME, config=model_config
    )
    
    # ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸° ì¡°ì •(í† í°ì„ ëŠ˜ë ¸ê¸° ë•Œë¬¸ì—)
    model.resize_token_embeddings(len(tokenizer))
    print("ì„ë² ë”© ë ˆì´ì–´ í¬ê¸° ì¡°ì • ì™„ë£Œ")
    
    print("--- Modeling Done ---")
    return tokenizer, model

# K-Foldë¥¼ ìœ„í•œ í•¨ìˆ˜(load_tokenizer_and_model_for_train ëŒ€ì‹ )
def load_tokenizer_and_model_for_kfold(model_name):
    """
    K-Foldì˜ ê° foldë§ˆë‹¤ ìƒˆë¡œìš´ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    (ê¸°ì¡´ load_tokenizer_and_model_for_train í•¨ìˆ˜ì™€ ê±°ì˜ ë™ì¼)
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # ë¹„ì‹ë³„í™” í† í° ì¶”ê°€ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
    new_tokens = ['&name&', '&location&', '&affiliation&', '&company&', '&brand&', '&art&', '&other&', '&nama&', '&affifiation&', '&name', '&online-account&', '&compnay&', '&anme&', '& name&', '&address&', '&tel-num&', '&naem&']
    tokenizer.add_tokens(new_tokens)

    model_config = AutoConfig.from_pretrained(model_name)
    model_config.num_labels = 2

    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=model_config)
    model.resize_token_embeddings(len(tokenizer))
    
    return tokenizer, model

def load_model_for_inference(model_name,model_dir):
    """ì¶”ë¡ (infer)ì— í•„ìš”í•œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € load """
    # load tokenizer
    Tokenizer_NAME = model_name
    tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)

    # ë¹„ì‹ë³„í™”ëœ ë‹¨ì–´ í† í°ìœ¼ë¡œ ì¶”ê°€
    new_tokens = ['&name&', '&location&', '&affiliation&', '&company&', '&brand&', '&art&', '&other&', '&nama&', '&affifiation&', '&name', '&online-account&', '&compnay&', '&anme&', '& name&', '&address&', '&tel-num&', '&naem&']
    tokenizer.add_tokens(new_tokens)
    
    print("í† í° ì¶”ê°€ í™•ì¸2:", tokenizer.convert_tokens_to_ids(new_tokens)) # í† í°ì´ ì˜ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
    print("ëŠ˜ì–´ë‚œ í† í° í¬ê¸°2:", len(tokenizer)) # í† í° í¬ê¸° ëŠ˜ì–´ë‚œ ê²ƒ í™•ì¸
    
    ## load my model
    model = AutoModelForSequenceClassification.from_pretrained(model_dir)
    
    model.resize_token_embeddings(len(tokenizer))
    print("ì¶”ë¡ ìš© ì„ë² ë”© ë ˆì´ì–´ í¬ê¸° ì¡°ì • ì™„ë£Œ")

    return tokenizer, model

# beomi-KcELECTRA-base-v2022 ì‹¤í–‰í•  ë•Œ ì˜¤ë¥˜ë‚˜ì„œ ì¶”ê°€í•¨
class ContiguousTrainer(Trainer):
    def _save(self, output_dir=None, state_dict=None):
        # contiguous ì¶”ê°€
        for name, param in self.model.named_parameters():
            if not param.is_contiguous():
                param.data = param.data.contiguous()
        super()._save(output_dir, state_dict)


def load_trainer_for_train(args, model, hate_train_dataset, hate_valid_dataset):
    """í•™ìŠµ(train)ì„ ìœ„í•œ huggingface trainer ì„¤ì •"""
    training_args = TrainingArguments(
        output_dir=args.save_path + "/results",  # output directory
        save_total_limit=args.save_limit,  # number of total save model.
        save_steps=args.save_step,  # model saving step.
        num_train_epochs=args.epochs,  # total number of training epochs
        learning_rate=args.lr,  # learning_rate
        per_device_train_batch_size=args.batch_size,  # batch size per device during training
        per_device_eval_batch_size=8,  # batch size for evaluation
        warmup_steps=args.warmup_steps,  # number of warmup steps for learning rate scheduler
        weight_decay=args.weight_decay,  # strength of weight decay
        logging_dir=args.save_path + "logs",  # directory for storing logs
        logging_steps=args.logging_step,  # log saving step.
        eval_strategy="steps",  # eval strategy to adopt during training
        # `no`: No evaluation during training.
        # `steps`: Evaluate every `eval_steps`.
        # `epoch`: Evaluate every end of epoch.
        eval_steps=args.eval_step,  # evaluation step.
        load_best_model_at_end=True,
        report_to="wandb",  # W&B ë¡œê¹… í™œì„±í™”
        run_name=args.run_name,  # run_name ì§€ì •
    )

    ## Add callback & optimizer & scheduler
    MyCallback = EarlyStoppingCallback(
        early_stopping_patience=3, early_stopping_threshold=0.001
    )

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        betas=(0.9, 0.999),
        eps=1e-08,
        weight_decay=args.weight_decay,
        amsgrad=False,
    )
    print("--- Set training arguments Done ---")

    trainer = ContiguousTrainer(
        model=model,  # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,  # training arguments, defined above
        train_dataset=hate_train_dataset,  # training dataset
        eval_dataset=hate_valid_dataset,  # evaluation dataset
        compute_metrics=compute_metrics,  # define metrics function
        callbacks=[MyCallback],
        optimizers=(
            optimizer,
            get_cosine_with_hard_restarts_schedule_with_warmup(
                optimizer,
                num_warmup_steps=args.warmup_steps,
                num_training_steps=len(hate_train_dataset) * args.epochs,
            ),
        ),
    )
    print("--- Set Trainer Done ---")

    return trainer


def train(args):
    """ëª¨ë¸ì„ í•™ìŠµ(train)í•˜ê³  best modelì„ ì €ì¥"""
    # fix a seed
    pl.seed_everything(seed=42, workers=False)

    # set device
    # device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("device:", device)

    if args.n_splits > 1:
        print(f"--- Starting {args.n_splits}-Fold Cross-Validation ---")
        
        # 1. K-Foldë¥¼ ìœ„í•œ ì „ì²´ ë°ì´í„° ë¡œë“œ
        DATASET_REVISION = args.revision
        full_train_df, _ = prepare_kfold_dataset(args.dataset_name, DATASET_REVISION)

        # 2. StratifiedKFold ì„¤ì •
        skf = StratifiedKFold(n_splits=args.n_splits, shuffle=True, random_state=args.seed)
        labels = full_train_df["output"].values
        
        all_fold_metrics = []

        # 3. K-Fold ë£¨í”„ ì‹œì‘
        for fold, (train_idx, val_idx) in enumerate(skf.split(full_train_df, labels)):
            print(f"\n========== Fold {fold + 1}/{args.n_splits} ==========")

            # 4. Foldë§ˆë‹¤ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ìƒˆë¡œ ë¡œë“œ
            tokenizer, model = load_tokenizer_and_model_for_kfold(args.model_name)
            model.to(device)

            # 5. í˜„ì¬ Foldì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ë¶„í•  ë° Pytorch Dataset ìƒì„±
            train_df = full_train_df.iloc[train_idx]
            valid_df = full_train_df.iloc[val_idx]
            
            tokenized_train = construct_tokenized_dataset(train_df, tokenizer, args.max_len, args.model_name)
            tokenized_valid = construct_tokenized_dataset(valid_df, tokenizer, args.max_len, args.model_name)

            hate_train_dataset = hate_dataset(tokenized_train, train_df["output"].values)
            hate_valid_dataset = hate_dataset(tokenized_valid, valid_df["output"].values)
            
            # 6. Foldë³„ TrainingArguments ì„¤ì •
            fold_output_dir = os.path.join(args.save_path, f"fold_{fold+1}")
            fold_run_name = f"{args.run_name}_fold_{fold+1}"

            training_args = TrainingArguments(
                output_dir=fold_output_dir,
                run_name=fold_run_name,
                # ... (ë‚˜ë¨¸ì§€ ì¸ìë“¤ì€ ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •) ...
                num_train_epochs=args.epochs,
                per_device_train_batch_size=args.batch_size,
                learning_rate=args.lr,
                weight_decay=args.weight_decay,
                warmup_steps=args.warmup_steps,
                logging_dir=os.path.join(fold_output_dir, "logs"),
                logging_steps=args.logging_step,
                eval_strategy="steps",
                eval_steps=args.eval_step,
                save_steps=args.save_step,
                save_total_limit=args.save_limit,
                load_best_model_at_end=True,
                metric_for_best_model="f1",
                report_to="wandb",
            )
            
            # Trainer ì„¤ì • (ContiguousTrainer ì‚¬ìš©)
            trainer = ContiguousTrainer(
                model=model,
                args=training_args,
                train_dataset=hate_train_dataset,
                eval_dataset=hate_valid_dataset,
                compute_metrics=compute_metrics,
                callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
            )

            # 7. í˜„ì¬ Fold í•™ìŠµ ì‹œì‘
            trainer.train()

            # 8. Fold í‰ê°€ ë° ê²°ê³¼ ì €ì¥
            metrics = trainer.evaluate(eval_dataset=hate_valid_dataset)
            all_fold_metrics.append(metrics)
            
            # 9. Foldë³„ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥
            best_model_path = os.path.join(args.model_dir, f"best_model_fold_{fold+1}")
            trainer.save_model(best_model_path)
            tokenizer.save_pretrained(best_model_path)
            print(f"Best model for fold {fold+1} saved to {best_model_path}")

        # 10. ìµœì¢… í‰ê·  ì ìˆ˜ ì¶œë ¥
        print("\n========== K-Fold Cross-Validation Summary ==========")
        avg_eval_f1 = np.mean([m["eval_f1"] for m in all_fold_metrics])
        print(f"Average F1 Score across {args.n_splits} folds: {avg_eval_f1:.4f}")

    # ==================================================================
    # ## ì¼ë°˜ í•™ìŠµ ë¡œì§ (n_splits <= 1)
    # ==================================================================
    else:
        print("--- Starting Single Training Run (K-Fold is not used) ---")

        # 1. ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer, model = load_tokenizer_and_model_for_train(args)
        model.to(device)

        # 2. ë°ì´í„°ì…‹ ì¤€ë¹„
        hate_train_dataset, hate_valid_dataset, _, _ = prepare_dataset(
            args.dataset_name, tokenizer, args.max_len, args.model_name, args.revision
        )

        # 3. Trainer ì¤€ë¹„
        trainer = load_trainer_for_train(
            args, model, hate_train_dataset, hate_valid_dataset
        )

        # 4. í•™ìŠµ ì‹œì‘
        trainer.train()
        print("--- Finish train ---")
        
        # 5. ìµœì¢… ëª¨ë¸ ì €ì¥
        model.save_pretrained(args.model_dir)
        tokenizer.save_pretrained(args.model_dir)